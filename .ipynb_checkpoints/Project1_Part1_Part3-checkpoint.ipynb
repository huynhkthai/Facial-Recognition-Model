{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part1:\n",
    "\n",
    "Hypotheses:\n",
    "    \n",
    "1. If the Asian population is completely removed from the training and evaluation set but is on the test set, the gender classification in as a whole would perform less well but not to the extend of how poorly it would be on the Asian and would be somewhat proportional to the percentage of the training set decrease.\n",
    "2. Since others population is rather small, if removing all men population of race other(about 3.5% of the total population) from the training data, it will not affect the performance of gender classification as a whole but will have very significant negative effect on gender classification performance on men of race other.\n",
    "3. If data trained on all races and genders are balanced, the performance of each subgroup should be similar even age distribution might be different between races and genders. However, for the subgroup to be evenly distributed, each group would have the population of the most minority group. Thus the general performance would be reduced proportionally to how much less the total data is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In hypothesis 1, we found that even when the Asian population was removed from the training set, the model's generally performed as well as if it did on baseline, strangely, the model also performed as well on all races including asian. However, looking at the incorrect prediction and the total distribution, asian, in percentage grew 80%, this is what we expected.\n",
    "On the other hand, for hypothesis 2, although the general performance loss is small, which is expected, when we removed other race men(at a much less percentage compared to asian) from the training data, we found a more significant performance loss in men prediction, in other words, the model made more incorrect prediction on men. But as expected, when removed men from other race, the model did much worse on men than on women.\n",
    "Finally, for hypothesis 3, we explored the possibility of even gender and race distribution. Although the training data set is significantly less(around 35% of the baseline), where was only 3.5% loss in general performance but the performance across all races and gender was similar and any deviation would be minimal and from age distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, when the data is skewed, the model works better on what it is trained more on and would work equally well on all gender and race if the input is evenly distributed. Unfortunately, this is not the case in reality. When AI is used in policing, the under-privilege, low-income dweller population gets profiled more and the model would work better on this population compared to the rest which would result in a negative cycle where this minority is pressed on more and is less likely to go up the ladder in the society even if there's efforts trying to break out. Many other application of AI also has similar issues. AI should serve everyone equally well rather than discriminating and targeting just a selected(conciously or not) audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
